{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XRIIT43SXR2",
        "outputId": "1dbcf27d-245e-45a5-f66a-51f51d8f0f37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-Qt8CAnD1Fm",
        "outputId": "a33a8063-8067-4356-ec08-99c15b3115c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['solar', 'system', 'gravitationally', 'sun', 'object', 'orbit', 'formed', 'billion', 'year', 'region', 'cloud', 'disc', 'star', 'fusion', 'hydrogen', 'helium', 'core', 'outer', 'largest', 'planet', 'four', 'terrestrial', 'mercury', 'venus', 'earth', 'mar', 'two', 'gas', 'giant', 'jupiter', 'saturn', 'ice', 'uranus', 'neptune', 'solid', 'surface', 'mainly', 'composed', 'mass', 'strong', 'astronomer', 'dwarf', 'pluto', 'makemake', 'small', 'body', 'asteroid', 'comet', 'centaur', 'meteoroid', 'dust', 'belt', 'kuiper', 'orbiting', 'natural', 'satellite', 'called', 'moon', 'particle', 'wind', 'heliosphere', 'around', 'interstellar', 'space', 'theorized', 'oort', 'source', 'radius', 'lightyears', 'closest', 'centauri', 'au', 'away', 'milky', 'way', 'gravitational', 'within', 'large', 'likely', 'one', 'mostly', 'heavier', 'element', 'center', 'diameter', 'roughly', 'km', 'mi', 'form', 'larger', 'may', 'ejected', 'due', 'point', 'metal', 'silicate', 'could', 'inner', 'close', 'would', 'beyond', 'material', 'enough', 'volatile', 'icy', 'massive', 'atmosphere', 'million', 'pressure', 'mainsequence', 'life', 'temperature', 'relatively', 'although', 'collision', 'much', 'known', 'time', 'greater', 'red', 'size', 'planetary', 'like', 'medium', 'structure', 'part', 'consisting', 'le', 'light', 'closer', 'affected', 'farther', 'lie', 'distance', 'near', 'plane', 'ecliptic', 'smaller', 'ring', 'direction', 'first', 'kepler', 'law', 'motion', 'cause', 'perihelion', 'whereas', 'distant', 'aphelion', 'many', 'scale', 'model', 'orbital', 'dominated', 'interaction', 'water', 'three', 'might', 'magnetosphere', 'magnetic', 'field', 'far', 'make', 'population', 'spiral', 'arm', 'galactic', 'thought', 'activity', 'similar', 'impact', 'crater', 'group', 'thousand', 'spacecraft', 'trojan', 'discovered', 'confirmed', 'extreme', 'still', 'whose', 'scattered', 'resonance', 'every', 'transneptunian', 'inclined', 'bubble', 'observation', 'local', 'maneuver']\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 7s 183ms/step - loss: 5.3529\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 5.2277\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 5.2225\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 5.2028\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 5.1696\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 5.0756\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 5.0708\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 4.9753\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 1s 110ms/step - loss: 4.8783\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 2s 201ms/step - loss: 4.6945\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 2s 198ms/step - loss: 4.4475\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 4.2067\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 3.9773\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 3.7522\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 1s 106ms/step - loss: 3.5145\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 1s 106ms/step - loss: 3.2753\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 3.0648\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 1s 96ms/step - loss: 2.8367\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 2.5931\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 2s 207ms/step - loss: 2.3587\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 2s 158ms/step - loss: 2.1618\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 1.9305\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 1.7549\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 1.5523\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 1.3878\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 1s 118ms/step - loss: 1.2210\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 2s 138ms/step - loss: 1.0854\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.9536\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 2s 176ms/step - loss: 0.8495\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 3s 210ms/step - loss: 0.7264\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 1s 112ms/step - loss: 0.6308\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.5797\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4886\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4420\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 1s 96ms/step - loss: 0.3834\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.3311\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.2902\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.2399\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 2s 171ms/step - loss: 0.2228\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 3s 216ms/step - loss: 0.2014\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.1789\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.1671\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.1491\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.1304\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.1309\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.1205\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.1112\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.1008\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 2s 173ms/step - loss: 0.0914\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 3s 219ms/step - loss: 0.0850\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.0749\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.0711\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.0684\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.0611\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 1s 96ms/step - loss: 0.0572\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.0533\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.0521\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 1s 107ms/step - loss: 0.0489\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 2s 185ms/step - loss: 0.0448\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 2s 201ms/step - loss: 0.0416\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.0390\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.0346\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.0368\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.0349\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 1s 106ms/step - loss: 0.0342\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.0300\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.0284\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.0295\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 2s 194ms/step - loss: 0.0257\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 2s 168ms/step - loss: 0.0266\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.0258\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.0247\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.0263\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.0223\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.0208\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 1s 110ms/step - loss: 0.0223\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.0210\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 1s 125ms/step - loss: 0.0198\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 2s 201ms/step - loss: 0.0201\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 2s 150ms/step - loss: 0.0187\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.0181\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.0166\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.0170\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.0172\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.0146\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.0153\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.0153\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 2s 146ms/step - loss: 0.0169\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 2s 205ms/step - loss: 0.0138\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 2s 124ms/step - loss: 0.0136\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.0135\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 1s 96ms/step - loss: 0.0144\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.0135\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.0145\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.0139\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.0126\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.0135\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 2s 181ms/step - loss: 0.0119\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 3s 211ms/step - loss: 0.0124\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.0128\n",
            "Epoch 1/100\n",
            "219/219 [==============================] - 29s 119ms/step - loss: 2.9642\n",
            "Epoch 2/100\n",
            "219/219 [==============================] - 26s 118ms/step - loss: 2.3651\n",
            "Epoch 3/100\n",
            "219/219 [==============================] - 26s 116ms/step - loss: 2.0885\n",
            "Epoch 4/100\n",
            "219/219 [==============================] - 24s 111ms/step - loss: 1.9264\n",
            "Epoch 5/100\n",
            "219/219 [==============================] - 26s 120ms/step - loss: 1.8120\n",
            "Epoch 6/100\n",
            "219/219 [==============================] - 26s 118ms/step - loss: 1.7304\n",
            "Epoch 7/100\n",
            "219/219 [==============================] - 25s 116ms/step - loss: 1.6705\n",
            "Epoch 8/100\n",
            "219/219 [==============================] - 26s 120ms/step - loss: 1.6161\n",
            "Epoch 9/100\n",
            "219/219 [==============================] - 25s 116ms/step - loss: 1.5802\n",
            "Epoch 10/100\n",
            "219/219 [==============================] - 25s 115ms/step - loss: 1.5412\n",
            "Epoch 11/100\n",
            "219/219 [==============================] - 26s 120ms/step - loss: 1.5145\n",
            "Epoch 12/100\n",
            "219/219 [==============================] - 26s 120ms/step - loss: 1.4889\n",
            "Epoch 13/100\n",
            "219/219 [==============================] - 26s 120ms/step - loss: 1.4684\n",
            "Epoch 14/100\n",
            "219/219 [==============================] - 27s 123ms/step - loss: 1.4477\n",
            "Epoch 15/100\n",
            "219/219 [==============================] - 25s 116ms/step - loss: 1.4269\n",
            "Epoch 16/100\n",
            "219/219 [==============================] - 25s 113ms/step - loss: 1.4125\n",
            "Epoch 17/100\n",
            "219/219 [==============================] - 26s 120ms/step - loss: 1.4039\n",
            "Epoch 18/100\n",
            "219/219 [==============================] - 26s 120ms/step - loss: 1.3856\n",
            "Epoch 19/100\n",
            "219/219 [==============================] - 26s 117ms/step - loss: 1.3771\n",
            "Epoch 20/100\n",
            "219/219 [==============================] - 26s 118ms/step - loss: 1.3635\n",
            "Epoch 21/100\n",
            "219/219 [==============================] - 24s 108ms/step - loss: 1.3594\n",
            "Epoch 22/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3497\n",
            "Epoch 23/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3383\n",
            "Epoch 24/100\n",
            "219/219 [==============================] - 26s 121ms/step - loss: 1.3274\n",
            "Epoch 25/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3267\n",
            "Epoch 26/100\n",
            "219/219 [==============================] - 25s 115ms/step - loss: 1.3225\n",
            "Epoch 27/100\n",
            "219/219 [==============================] - 25s 115ms/step - loss: 1.3126\n",
            "Epoch 28/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3149\n",
            "Epoch 29/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3112\n",
            "Epoch 30/100\n",
            "219/219 [==============================] - 26s 117ms/step - loss: 1.3122\n",
            "Epoch 31/100\n",
            "219/219 [==============================] - 26s 118ms/step - loss: 1.3033\n",
            "Epoch 32/100\n",
            "219/219 [==============================] - 24s 111ms/step - loss: 1.2972\n",
            "Epoch 33/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3006\n",
            "Epoch 34/100\n",
            "219/219 [==============================] - 26s 120ms/step - loss: 1.2975\n",
            "Epoch 35/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.2895\n",
            "Epoch 36/100\n",
            "219/219 [==============================] - 25s 115ms/step - loss: 1.2963\n",
            "Epoch 37/100\n",
            "219/219 [==============================] - 24s 111ms/step - loss: 1.2931\n",
            "Epoch 38/100\n",
            "219/219 [==============================] - 27s 121ms/step - loss: 1.2887\n",
            "Epoch 39/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.2883\n",
            "Epoch 40/100\n",
            "219/219 [==============================] - 26s 120ms/step - loss: 1.2874\n",
            "Epoch 41/100\n",
            "219/219 [==============================] - 26s 121ms/step - loss: 1.2918\n",
            "Epoch 42/100\n",
            "219/219 [==============================] - 24s 109ms/step - loss: 1.2959\n",
            "Epoch 43/100\n",
            "219/219 [==============================] - 26s 116ms/step - loss: 1.2927\n",
            "Epoch 44/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.2876\n",
            "Epoch 45/100\n",
            "219/219 [==============================] - 26s 120ms/step - loss: 1.2910\n",
            "Epoch 46/100\n",
            "219/219 [==============================] - 26s 120ms/step - loss: 1.2859\n",
            "Epoch 47/100\n",
            "219/219 [==============================] - 25s 113ms/step - loss: 1.2900\n",
            "Epoch 48/100\n",
            "219/219 [==============================] - 25s 112ms/step - loss: 1.2917\n",
            "Epoch 49/100\n",
            "219/219 [==============================] - 26s 121ms/step - loss: 1.2860\n",
            "Epoch 50/100\n",
            "219/219 [==============================] - 27s 123ms/step - loss: 1.2911\n",
            "Epoch 51/100\n",
            "219/219 [==============================] - 27s 123ms/step - loss: 1.2956\n",
            "Epoch 52/100\n",
            "219/219 [==============================] - 27s 124ms/step - loss: 1.2976\n",
            "Epoch 53/100\n",
            "219/219 [==============================] - 27s 124ms/step - loss: 1.3045\n",
            "Epoch 54/100\n",
            "219/219 [==============================] - 27s 122ms/step - loss: 1.2917\n",
            "Epoch 55/100\n",
            "219/219 [==============================] - 26s 118ms/step - loss: 1.2958\n",
            "Epoch 56/100\n",
            "219/219 [==============================] - 25s 113ms/step - loss: 1.2978\n",
            "Epoch 57/100\n",
            "219/219 [==============================] - 26s 120ms/step - loss: 1.3049\n",
            "Epoch 58/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3040\n",
            "Epoch 59/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3106\n",
            "Epoch 60/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3016\n",
            "Epoch 61/100\n",
            "219/219 [==============================] - 25s 112ms/step - loss: 1.3073\n",
            "Epoch 62/100\n",
            "219/219 [==============================] - 26s 117ms/step - loss: 1.3018\n",
            "Epoch 63/100\n",
            "219/219 [==============================] - 26s 117ms/step - loss: 1.3126\n",
            "Epoch 64/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3066\n",
            "Epoch 65/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3181\n",
            "Epoch 66/100\n",
            "219/219 [==============================] - 24s 111ms/step - loss: 1.3248\n",
            "Epoch 67/100\n",
            "219/219 [==============================] - 25s 116ms/step - loss: 1.3259\n",
            "Epoch 68/100\n",
            "219/219 [==============================] - 25s 115ms/step - loss: 1.3239\n",
            "Epoch 69/100\n",
            "219/219 [==============================] - 26s 117ms/step - loss: 1.3206\n",
            "Epoch 70/100\n",
            "219/219 [==============================] - 26s 118ms/step - loss: 1.3180\n",
            "Epoch 71/100\n",
            "219/219 [==============================] - 24s 112ms/step - loss: 1.3181\n",
            "Epoch 72/100\n",
            "219/219 [==============================] - 25s 115ms/step - loss: 1.3295\n",
            "Epoch 73/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3362\n",
            "Epoch 74/100\n",
            "219/219 [==============================] - 26s 117ms/step - loss: 1.3302\n",
            "Epoch 75/100\n",
            "219/219 [==============================] - 26s 118ms/step - loss: 1.3342\n",
            "Epoch 76/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3392\n",
            "Epoch 77/100\n",
            "219/219 [==============================] - 24s 110ms/step - loss: 1.3434\n",
            "Epoch 78/100\n",
            "219/219 [==============================] - 26s 117ms/step - loss: 1.3552\n",
            "Epoch 79/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3419\n",
            "Epoch 80/100\n",
            "219/219 [==============================] - 26s 120ms/step - loss: 1.3569\n",
            "Epoch 81/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3430\n",
            "Epoch 82/100\n",
            "219/219 [==============================] - 24s 108ms/step - loss: 1.3517\n",
            "Epoch 83/100\n",
            "219/219 [==============================] - 26s 117ms/step - loss: 1.3573\n",
            "Epoch 84/100\n",
            "219/219 [==============================] - 27s 122ms/step - loss: 1.3571\n",
            "Epoch 85/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3591\n",
            "Epoch 86/100\n",
            "219/219 [==============================] - 26s 121ms/step - loss: 1.3655\n",
            "Epoch 87/100\n",
            "219/219 [==============================] - 25s 116ms/step - loss: 1.3708\n",
            "Epoch 88/100\n",
            "219/219 [==============================] - 25s 112ms/step - loss: 1.3732\n",
            "Epoch 89/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3815\n",
            "Epoch 90/100\n",
            "219/219 [==============================] - 26s 118ms/step - loss: 1.3864\n",
            "Epoch 91/100\n",
            "219/219 [==============================] - 26s 118ms/step - loss: 1.3778\n",
            "Epoch 92/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.3841\n",
            "Epoch 93/100\n",
            "219/219 [==============================] - 25s 113ms/step - loss: 1.3912\n",
            "Epoch 94/100\n",
            "219/219 [==============================] - 25s 114ms/step - loss: 1.3970\n",
            "Epoch 95/100\n",
            "219/219 [==============================] - 27s 122ms/step - loss: 1.4004\n",
            "Epoch 96/100\n",
            "219/219 [==============================] - 27s 121ms/step - loss: 1.3967\n",
            "Epoch 97/100\n",
            "219/219 [==============================] - 26s 119ms/step - loss: 1.4042\n",
            "Epoch 98/100\n",
            "219/219 [==============================] - 26s 118ms/step - loss: 1.4036\n",
            "Epoch 99/100\n",
            "219/219 [==============================] - 25s 112ms/step - loss: 1.4061\n",
            "Epoch 100/100\n",
            "219/219 [==============================] - 25s 114ms/step - loss: 1.4114\n",
            "1/1 [==============================] - 0s 363ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 352ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "Generated text using character-based model:\n",
            "The Solar System is the gravitational an\n",
            "\n",
            "Generated text using word-based model:\n",
            "solar system gravitationally bound system sun object orbit formed billion year ago dense region molecular cloud collapsed forming sun protoplanetary disc sun ordinary main sequence star maintains balanced equilibrium fusion helium star sun sun whereas first time outer life make\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia\n",
        "import numpy as np\n",
        "import random\n",
        "import wikipedia\n",
        "import nltk\n",
        "import re\n",
        "from gensim.models import KeyedVectors  # for GloVe\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Dropout, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# Set the language of Wikipedia to English\n",
        "wikipedia.set_lang(\"en\")\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Preprocessing Steps\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Define the topic\n",
        "topic = \"The solar system\"\n",
        "\n",
        "# Fetch Wikipedia page for the topic\n",
        "try:\n",
        "    page = wikipedia.page(topic)\n",
        "    content = page.content\n",
        "    # Preprocess content\n",
        "    content_tokens = preprocess_text(content)\n",
        "except wikipedia.exceptions.PageError:\n",
        "    print(\"Wikipedia page for the given topic is not found.\")\n",
        "    exit(1)\n",
        "\n",
        "# Splitting content into 5 documents\n",
        "num_documents = 5\n",
        "doc_length = len(content_tokens) // num_documents\n",
        "# Slices the content_tokens list to extract tokens corresponding to the current document\n",
        "all_docs_tokens = [content_tokens[i * doc_length: (i + 1) * doc_length] for i in range(num_documents)]\n",
        "\n",
        "# GloVe embeddings -> pre-trained word vectors that capture semantic relationships between words\n",
        "\n",
        "# Load GloVe embeddings for words embeddings\n",
        "glove_file = '/content/drive/MyDrive/NLP(RNN)/glove.6B.100d.txt'\n",
        "glove_model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n",
        "embedding_dim = glove_model.vector_size    # retrieves the dimensionality of the word embeddings (size of them)\n",
        "\n",
        "# Define sequence lengths for training\n",
        "char_sequence_length = 30\n",
        "word_sequence_length = 30\n",
        "\n",
        "# Create a set of unique characters from the content (to ensure that each character appears only once in the set)\n",
        "chars = sorted(set([char.lower() for char in content if char in string.printable]))\n",
        "\n",
        "# Filter out rare words to reduce the vocabulary size and they are noise in the data and can lead to overfitting\n",
        "word_counts = nltk.FreqDist(word for doc in all_docs_tokens for word in doc)\n",
        "filtered_words = [word for word, count in word_counts.items() if count > 5]\n",
        "\n",
        "print(filtered_words)\n",
        "\n",
        "# Prepare input data for word-based model\n",
        "X_word = []\n",
        "y_word = []\n",
        "for doc_tokens in all_docs_tokens:\n",
        "    for i in range(len(doc_tokens) - word_sequence_length):\n",
        "        seq_in = doc_tokens[i:i + word_sequence_length]\n",
        "        seq_out = doc_tokens[i + word_sequence_length]\n",
        "        if seq_out in filtered_words:\n",
        "            X_word.append([glove_model[word] if word in glove_model else np.zeros(embedding_dim) for word in seq_in])\n",
        "            y_word.append(to_categorical(filtered_words.index(seq_out), num_classes=len(filtered_words)))\n",
        "\n",
        "X_word = np.array(X_word)\n",
        "y_word = np.array(y_word)\n",
        "\n",
        "# Build the word-based model with 4 SimpleRNN layers\n",
        "model_word = Sequential([\n",
        "    SimpleRNN(512, input_shape=(word_sequence_length, embedding_dim), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    SimpleRNN(512, return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    SimpleRNN(512, return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    SimpleRNN(512),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(filtered_words), activation='softmax')\n",
        "])\n",
        "model_word.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "# Train the word-based model\n",
        "model_word.fit(X_word, y_word, epochs=100, batch_size=256)\n",
        "\n",
        "# Prepare input data for character-based model\n",
        "X_char = []\n",
        "y_char = []\n",
        "for i in range(len(content) - char_sequence_length):\n",
        "    seq_in = content[i:i + char_sequence_length]\n",
        "    seq_out = content[i + char_sequence_length]\n",
        "    if seq_out in chars:  # Ensure the output character is in the list of characters\n",
        "        X_char.append([glove_model[char] if char in glove_model else np.zeros(embedding_dim) for char in seq_in])\n",
        "        y_char.append(to_categorical(chars.index(seq_out), num_classes=len(chars)))\n",
        "\n",
        "X_char = np.array(X_char)\n",
        "y_char = np.array(y_char)\n",
        "\n",
        "# Build the character-based model with 4 SimpleRNN layers\n",
        "model_char = Sequential([\n",
        "    SimpleRNN(512, input_shape=(char_sequence_length, embedding_dim), return_sequences=True),\n",
        "    Dropout(0.3),  # prevent overfitting\n",
        "    SimpleRNN(512, return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    SimpleRNN(512, return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    SimpleRNN(512),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(chars), activation='softmax')\n",
        "])\n",
        "model_char.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "# Train the character-based model\n",
        "model_char.fit(X_char, y_char, epochs=100, batch_size=256)\n",
        "\n",
        "# Temperature parameter -> controls the randomness or diversity of the generated text\n",
        "# Text generation function with temperature parameter for character\n",
        "def generate_text_char(seed_text, num_chars, temperature=1.0):  # num_chars -> the number of characters to generate\n",
        "    generated_text = seed_text\n",
        "    for _ in range(num_chars):\n",
        "        x_pred = np.zeros((1, char_sequence_length, embedding_dim))  # x_pred holds the input data for prediction(1 -> represent a single input sequence)\n",
        "        # Take a fixed-size window of characters from seed_text\n",
        "        for t, char in enumerate(seed_text[-char_sequence_length:]):\n",
        "            if char in glove_model:\n",
        "                x_pred[0, t] = glove_model[char]\n",
        "        preds = model_char.predict(x_pred)[0]\n",
        "        next_index = sample(preds, temperature)\n",
        "        next_char = chars[next_index]\n",
        "        generated_text += next_char\n",
        "        seed_text = seed_text[1:] + next_char  # Move the window by one character\n",
        "    return generated_text\n",
        "\n",
        "# Text generation function with temperature parameter for word\n",
        "def generate_text_word(seed_text, num_words, temperature=1.0):\n",
        "    generated_text = seed_text\n",
        "    for _ in range(num_words):\n",
        "        x_pred = np.zeros((1, word_sequence_length, embedding_dim))\n",
        "        seed_text_split = seed_text.split()[-word_sequence_length:]\n",
        "        for t, word in enumerate(seed_text_split):\n",
        "            if word in glove_model and word in filtered_words:\n",
        "                x_pred[0, t] = glove_model[word]\n",
        "        preds = model_word.predict(x_pred)[0]\n",
        "        next_index = sample(preds, temperature)\n",
        "        next_word = filtered_words[next_index]\n",
        "        generated_text += \" \" + next_word\n",
        "        seed_text = ' '.join(seed_text.split()[1:]) + \" \" + next_word\n",
        "    return generated_text\n",
        "\n",
        "# Function to select the next character or word based on the predicted probabilities generated by the model\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "# Generate text using character-based model for at least 10 iterations\n",
        "initial_char_seed = content[:char_sequence_length]\n",
        "generated_text_char = generate_text_char(initial_char_seed, 10, temperature=0.5)\n",
        "\n",
        "# Generate text using word-based model for at least 10 iterations\n",
        "initial_word_seed = ' '.join(content_tokens[:word_sequence_length])\n",
        "generated_text_word = generate_text_word(initial_word_seed, 10, temperature=0.5)\n",
        "\n",
        "print(\"Generated text using character-based model:\")\n",
        "print(generated_text_char)\n",
        "\n",
        "print(\"\\nGenerated text using word-based model:\")\n",
        "print(generated_text_word)\n"
      ]
    }
  ]
}